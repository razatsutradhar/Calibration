{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-win_amd64.whl (35.6 MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from opencv-python) (1.20.3)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: depthai in c:\\users\\razat\\anaconda3\\lib\\site-packages (2.19.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install depthai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: blobconverter in c:\\users\\razat\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\razat\\anaconda3\\lib\\site-packages (from blobconverter) (2.26.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from blobconverter) (1.26.22)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\razat\\anaconda3\\lib\\site-packages (from blobconverter) (6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from boto3->blobconverter) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from boto3->blobconverter) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.22 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from boto3->blobconverter) (1.29.22)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from botocore<1.30.0,>=1.29.22->boto3->blobconverter) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from botocore<1.30.0,>=1.29.22->boto3->blobconverter) (1.26.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.22->boto3->blobconverter) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from requests->blobconverter) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from requests->blobconverter) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\razat\\anaconda3\\lib\\site-packages (from requests->blobconverter) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install blobconverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import cv2\n",
    "import glob\n",
    "import depthai  # depthai - access the camera and its data packets\n",
    "import blobconverter  # blobconverter - compile and download MyriadX neural network blobs\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the camera\n",
    "pipeline = depthai.Pipeline()\n",
    "cam_rgb = pipeline.create(depthai.node.ColorCamera)\n",
    "cam_rgb.setPreviewSize(1280,720)\n",
    "cam_rgb.setInterleaved(False)\n",
    "\n",
    "\n",
    "detection_nn = pipeline.create(depthai.node.MobileNetDetectionNetwork)\n",
    "# Set path of the blob (NN model). We will use blobconverter to convert&download the model\n",
    "# detection_nn.setBlobPath(\"/path/to/model.blob\")\n",
    "detection_nn.setBlobPath(blobconverter.from_zoo(name='mobilenet-ssd', shaves=6))\n",
    "detection_nn.setConfidenceThreshold(0.5)\n",
    "\n",
    "#Creating a stream of the rgb camera and naming it \"rgb\"\n",
    "xout_rgb = pipeline.create(depthai.node.XLinkOut)\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "cam_rgb.preview.link(xout_rgb.input)\n",
    "\n",
    "#Creating a stream of the detection_nn and naming it \"nn\"\n",
    "xout_nn = pipeline.create(depthai.node.XLinkOut)\n",
    "xout_nn.setStreamName(\"nn\")\n",
    "detection_nn.out.link(xout_nn.input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "# Pipeline is now finished, and we need to find an available device to run our pipeline\n",
    "# we are using context manager here that will dispose the device after we stop using it\n",
    "with depthai.Device(pipeline) as device:\n",
    "    # From this point, the Device will be in \"running\" mode and will start sending data via XLink\n",
    "\n",
    "    # To consume the device results, we get two output queues from the device, with stream names we assigned earlier\n",
    "    q_rgb = device.getOutputQueue(\"rgb\")\n",
    "    q_nn = device.getOutputQueue(\"nn\")\n",
    "\n",
    "    # Here, some of the default values are defined. Frame will be an image from \"rgb\" stream, detections will contain nn results\n",
    "    frame = None\n",
    "    detections = []\n",
    "\n",
    "    # Since the detections returned by nn have values from <0..1> range, they need to be multiplied by frame width/height to\n",
    "    # receive the actual position of the bounding box on the image\n",
    "    def frameNorm(frame, bbox):\n",
    "        normVals = np.full(len(bbox), frame.shape[0])\n",
    "        normVals[::2] = frame.shape[1]\n",
    "        return (np.clip(np.array(bbox), 0, 1) * normVals).astype(int)\n",
    "\n",
    "    img_counter = 0\n",
    "\n",
    "    # Main host-side application loop\n",
    "    while True:\n",
    "        # we try to fetch the data from nn/rgb queues. tryGet will return either the data packet or None if there isn't any\n",
    "        in_rgb = q_rgb.tryGet()\n",
    "        in_nn = q_nn.tryGet()\n",
    "\n",
    "        if in_rgb is not None:\n",
    "            # If the packet from RGB camera is present, we're retrieving the frame in OpenCV format using getCvFrame\n",
    "            frame = in_rgb.getCvFrame()\n",
    "            # print(frame.shape)\n",
    "        if in_nn is not None:\n",
    "            # when data from nn is received, we take the detections array that contains mobilenet-ssd results\n",
    "            detections = in_nn.detections\n",
    "\n",
    "        if frame is not None:\n",
    "            for detection in detections:\n",
    "                # for each bounding box, we first normalize it to match the frame size\n",
    "                bbox = frameNorm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))\n",
    "                # and then draw a rectangle on the frame to show the actual result\n",
    "                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n",
    "            # After all the drawing is finished, we show the frame on the screen\n",
    "            cv2.imshow(\"preview\", frame)\n",
    "\n",
    "        # at any time, you can press \"q\" and exit the main loop, therefore exiting the program itself\n",
    "        k = cv.waitKey(1)\n",
    "        if k%256 == 27:\n",
    "            # ESC pressed\n",
    "            print(\"Escape hit, closing...\")\n",
    "            break\n",
    "        elif k%256 == 32:\n",
    "            # SPACE pressed\n",
    "            img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "            cv.imwrite(img_name, frame)\n",
    "            print(\"{} written!\".format(img_name))\n",
    "            img_counter += 1\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termination criteria\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*8,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:8,0:6].T.reshape(-1,2)\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "images = glob.glob('*.png')\n",
    "for fname in images:\n",
    "    img = cv.imread(fname)\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv.findChessboardCorners(gray, (8,6), None)\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        corners2 = cv.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria)\n",
    "        imgpoints.append(corners2)\n",
    "        # Draw and display the corners\n",
    "        cv.drawChessboardCorners(img, (8,6), corners2, ret)\n",
    "        cv.imshow('img', img)\n",
    "        cv.waitKey(500)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the calibration data to a file.\n",
    "calibration = {'ret': ret, 'mtx': mtx, 'dist': dist, 'rvecs': rvecs, 'tvecs': tvecs}\n",
    "with open('calibration.pkl', 'wb') as f:\n",
    "    pickle.dump(calibration, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the calibration data from a file.\n",
    "with open('calibration.pkl', 'rb') as f:\n",
    "    calibration = pickle.load(f)\n",
    "ret = calibration['ret']\n",
    "mtx = calibration['mtx']\n",
    "dist = calibration['dist']\n",
    "rvecs = calibration['rvecs']\n",
    "tvecs = calibration['tvecs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3) (27, 18, 1250, 678)\n"
     ]
    }
   ],
   "source": [
    "#Reading the image and getting the height and width of the image.\n",
    "img = cv.imread('opencv_frame_0.png')\n",
    "h,  w = img.shape[:2]\n",
    "\n",
    "#Getting the new camera matrix and undistorting the image.\n",
    "newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))\n",
    "print(newcameramtx.shape,roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(678, 1250, 3)\n"
     ]
    }
   ],
   "source": [
    "# undistort\n",
    "mapx, mapy = cv.initUndistortRectifyMap(mtx, dist, None, newcameramtx, (w,h), 5)\n",
    "dst = cv.remap(img, mapx, mapy, cv.INTER_LINEAR)\n",
    "# crop the image\n",
    "x, y, w, h = roi\n",
    "dst = dst[y:y+h, x:x+w]\n",
    "cv.imwrite('calibresult.png', dst)\n",
    "print(dst.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a2fc1ab052b5ea03d35687895d874e32b6880721a9faa143e63f12bf35bd2cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
